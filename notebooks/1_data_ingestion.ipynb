{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Data Ingestion and Storage Design\n", "Covers SparkSession config, validation, partitioned Parquet storage."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "import sys\n", "sys.path.append(str(Path('..').resolve()))\n", "from scripts.run_pipeline import create_spark_session, create_clean_table, download_dataset, ensure_dirs\n"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["BASE = Path('..').resolve()\n", "dirs = ensure_dirs(BASE)\n", "_ = download_dataset(dirs['json_chunks'], 50000, 100000, None, 60, None, dirs['reports']/ 'pipeline_lineage.jsonl')\n", "spark = create_spark_session(BASE, BASE / 'config' / 'spark_config.yaml')\n", "df = create_clean_table(spark, str(dirs['json_chunks'] / '*.json'), dirs['processed'] / 'crashes_clean', dirs['reports'], dirs['reports']/ 'pipeline_lineage.jsonl')\n", "print(df.count())\n", "spark.stop()\n"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}},
 "nbformat": 4,
 "nbformat_minor": 5
}
