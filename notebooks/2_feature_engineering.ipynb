{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Distributed Data Processing Pipeline\n", "Covers broadcast join, cache strategy, lineage outputs."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "import sys\n", "sys.path.append(str(Path('..').resolve()))\n", "from scripts.run_pipeline import analyze, create_spark_session, ensure_dirs\n"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["BASE = Path('..').resolve()\n", "dirs = ensure_dirs(BASE)\n", "spark = create_spark_session(BASE, BASE / 'config' / 'spark_config.yaml')\n", "clean_df = spark.read.parquet(str(dirs['processed'] / 'crashes_clean'))\n", "analyze(clean_df, dirs['reports'], dirs['figures'], dirs['reports'] / 'pipeline_lineage.jsonl')\n", "clean_df.explain()\n", "spark.stop()\n"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}},
 "nbformat": 4,
 "nbformat_minor": 5
}
